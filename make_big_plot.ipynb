{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b26f4d3",
   "metadata": {},
   "source": [
    "# Make Big Plot \n",
    "\n",
    "Code for combining all data to do with wildfire DRF effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7add688",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# display all columns\n",
    "pd.options.display.max_columns = None\n",
    "pd.options.display.max_rows = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64ec0623",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function to_datetime in module pandas.core.tools.datetimes:\n",
      "\n",
      "to_datetime(arg: Union[~DatetimeScalar, List, Tuple, ~ArrayLike, ForwardRef('Series')], errors: str = 'raise', dayfirst: bool = False, yearfirst: bool = False, utc: Optional[bool] = None, format: Optional[str] = None, exact: bool = True, unit: Optional[str] = None, infer_datetime_format: bool = False, origin='unix', cache: bool = True) -> Union[pandas.core.indexes.datetimes.DatetimeIndex, ForwardRef('Series'), ~DatetimeScalar, ForwardRef('NaTType')]\n",
      "    Convert argument to datetime.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    arg : int, float, str, datetime, list, tuple, 1-d array, Series, DataFrame/dict-like\n",
      "        The object to convert to a datetime.\n",
      "    errors : {'ignore', 'raise', 'coerce'}, default 'raise'\n",
      "        - If 'raise', then invalid parsing will raise an exception.\n",
      "        - If 'coerce', then invalid parsing will be set as NaT.\n",
      "        - If 'ignore', then invalid parsing will return the input.\n",
      "    dayfirst : bool, default False\n",
      "        Specify a date parse order if `arg` is str or its list-likes.\n",
      "        If True, parses dates with the day first, eg 10/11/12 is parsed as\n",
      "        2012-11-10.\n",
      "        Warning: dayfirst=True is not strict, but will prefer to parse\n",
      "        with day first (this is a known bug, based on dateutil behavior).\n",
      "    yearfirst : bool, default False\n",
      "        Specify a date parse order if `arg` is str or its list-likes.\n",
      "    \n",
      "        - If True parses dates with the year first, eg 10/11/12 is parsed as\n",
      "          2010-11-12.\n",
      "        - If both dayfirst and yearfirst are True, yearfirst is preceded (same\n",
      "          as dateutil).\n",
      "    \n",
      "        Warning: yearfirst=True is not strict, but will prefer to parse\n",
      "        with year first (this is a known bug, based on dateutil behavior).\n",
      "    utc : bool, default None\n",
      "        Return UTC DatetimeIndex if True (converting any tz-aware\n",
      "        datetime.datetime objects as well).\n",
      "    format : str, default None\n",
      "        The strftime to parse time, eg \"%d/%m/%Y\", note that \"%f\" will parse\n",
      "        all the way up to nanoseconds.\n",
      "        See strftime documentation for more information on choices:\n",
      "        https://docs.python.org/3/library/datetime.html#strftime-and-strptime-behavior.\n",
      "    exact : bool, True by default\n",
      "        Behaves as:\n",
      "        - If True, require an exact format match.\n",
      "        - If False, allow the format to match anywhere in the target string.\n",
      "    \n",
      "    unit : str, default 'ns'\n",
      "        The unit of the arg (D,s,ms,us,ns) denote the unit, which is an\n",
      "        integer or float number. This will be based off the origin.\n",
      "        Example, with unit='ms' and origin='unix' (the default), this\n",
      "        would calculate the number of milliseconds to the unix epoch start.\n",
      "    infer_datetime_format : bool, default False\n",
      "        If True and no `format` is given, attempt to infer the format of the\n",
      "        datetime strings based on the first non-NaN element,\n",
      "        and if it can be inferred, switch to a faster method of parsing them.\n",
      "        In some cases this can increase the parsing speed by ~5-10x.\n",
      "    origin : scalar, default 'unix'\n",
      "        Define the reference date. The numeric values would be parsed as number\n",
      "        of units (defined by `unit`) since this reference date.\n",
      "    \n",
      "        - If 'unix' (or POSIX) time; origin is set to 1970-01-01.\n",
      "        - If 'julian', unit must be 'D', and origin is set to beginning of\n",
      "          Julian Calendar. Julian day number 0 is assigned to the day starting\n",
      "          at noon on January 1, 4713 BC.\n",
      "        - If Timestamp convertible, origin is set to Timestamp identified by\n",
      "          origin.\n",
      "    cache : bool, default True\n",
      "        If True, use a cache of unique, converted dates to apply the datetime\n",
      "        conversion. May produce significant speed-up when parsing duplicate\n",
      "        date strings, especially ones with timezone offsets. The cache is only\n",
      "        used when there are at least 50 values. The presence of out-of-bounds\n",
      "        values will render the cache unusable and may slow down parsing.\n",
      "    \n",
      "        .. versionchanged:: 0.25.0\n",
      "            - changed default value from False to True.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    datetime\n",
      "        If parsing succeeded.\n",
      "        Return type depends on input:\n",
      "    \n",
      "        - list-like: DatetimeIndex\n",
      "        - Series: Series of datetime64 dtype\n",
      "        - scalar: Timestamp\n",
      "    \n",
      "        In case when it is not possible to return designated types (e.g. when\n",
      "        any element of input is before Timestamp.min or after Timestamp.max)\n",
      "        return will have datetime.datetime type (or corresponding\n",
      "        array/Series).\n",
      "    \n",
      "    See Also\n",
      "    --------\n",
      "    DataFrame.astype : Cast argument to a specified dtype.\n",
      "    to_timedelta : Convert argument to timedelta.\n",
      "    convert_dtypes : Convert dtypes.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    Assembling a datetime from multiple columns of a DataFrame. The keys can be\n",
      "    common abbreviations like ['year', 'month', 'day', 'minute', 'second',\n",
      "    'ms', 'us', 'ns']) or plurals of the same\n",
      "    \n",
      "    >>> df = pd.DataFrame({'year': [2015, 2016],\n",
      "    ...                    'month': [2, 3],\n",
      "    ...                    'day': [4, 5]})\n",
      "    >>> pd.to_datetime(df)\n",
      "    0   2015-02-04\n",
      "    1   2016-03-05\n",
      "    dtype: datetime64[ns]\n",
      "    \n",
      "    If a date does not meet the `timestamp limitations\n",
      "    <https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html\n",
      "    #timeseries-timestamp-limits>`_, passing errors='ignore'\n",
      "    will return the original input instead of raising any exception.\n",
      "    \n",
      "    Passing errors='coerce' will force an out-of-bounds date to NaT,\n",
      "    in addition to forcing non-dates (or non-parseable dates) to NaT.\n",
      "    \n",
      "    >>> pd.to_datetime('13000101', format='%Y%m%d', errors='ignore')\n",
      "    datetime.datetime(1300, 1, 1, 0, 0)\n",
      "    >>> pd.to_datetime('13000101', format='%Y%m%d', errors='coerce')\n",
      "    NaT\n",
      "    \n",
      "    Passing infer_datetime_format=True can often-times speedup a parsing\n",
      "    if its not an ISO8601 format exactly, but in a regular format.\n",
      "    \n",
      "    >>> s = pd.Series(['3/11/2000', '3/12/2000', '3/13/2000'] * 1000)\n",
      "    >>> s.head()\n",
      "    0    3/11/2000\n",
      "    1    3/12/2000\n",
      "    2    3/13/2000\n",
      "    3    3/11/2000\n",
      "    4    3/12/2000\n",
      "    dtype: object\n",
      "    \n",
      "    >>> %timeit pd.to_datetime(s, infer_datetime_format=True)  # doctest: +SKIP\n",
      "    100 loops, best of 3: 10.4 ms per loop\n",
      "    \n",
      "    >>> %timeit pd.to_datetime(s, infer_datetime_format=False)  # doctest: +SKIP\n",
      "    1 loop, best of 3: 471 ms per loop\n",
      "    \n",
      "    Using a unix epoch time\n",
      "    \n",
      "    >>> pd.to_datetime(1490195805, unit='s')\n",
      "    Timestamp('2017-03-22 15:16:45')\n",
      "    >>> pd.to_datetime(1490195805433502912, unit='ns')\n",
      "    Timestamp('2017-03-22 15:16:45.433502912')\n",
      "    \n",
      "    .. warning:: For float arg, precision rounding might happen. To prevent\n",
      "        unexpected behavior use a fixed-width exact type.\n",
      "    \n",
      "    Using a non-unix epoch origin\n",
      "    \n",
      "    >>> pd.to_datetime([1, 2, 3], unit='D',\n",
      "    ...                origin=pd.Timestamp('1960-01-01'))\n",
      "    DatetimeIndex(['1960-01-02', '1960-01-03', '1960-01-04'], dtype='datetime64[ns]', freq=None)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# user inputs\n",
    "\n",
    "# enter study start and end dates, formatted \"dd/mm/yyyy\"\n",
    "interval_start = \"01/07/2015\"\n",
    "interval_end = \"01/09/2015\"\n",
    "\n",
    "help(pd.to_datetime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b67474b7",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "If using all scalar values, you must pass an index",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-220bdc9f5789>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0minterval\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdate_range\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfreq\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"1H\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0minterval\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minterval\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# saving as csv converts datetimes to str, needs to match other datasets\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0minterval_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m\"datetime\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0minterval\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;31m#interval_df\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniforge3\\envs\\drf2\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    527\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    528\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 529\u001b[1;33m             \u001b[0mmgr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minit_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    530\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMaskedArray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    531\u001b[0m             \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmrecords\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mmrecords\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniforge3\\envs\\drf2\\lib\\site-packages\\pandas\\core\\internals\\construction.py\u001b[0m in \u001b[0;36minit_dict\u001b[1;34m(data, index, columns, dtype)\u001b[0m\n\u001b[0;32m    285\u001b[0m             \u001b[0marr\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mis_datetime64tz_dtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0marr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0marr\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    286\u001b[0m         ]\n\u001b[1;32m--> 287\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marrays_to_mgr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_names\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    288\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    289\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniforge3\\envs\\drf2\\lib\\site-packages\\pandas\\core\\internals\\construction.py\u001b[0m in \u001b[0;36marrays_to_mgr\u001b[1;34m(arrays, arr_names, index, columns, dtype, verify_integrity)\u001b[0m\n\u001b[0;32m     78\u001b[0m         \u001b[1;31m# figure out the index, if necessary\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 80\u001b[1;33m             \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mextract_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     81\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m             \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mensure_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniforge3\\envs\\drf2\\lib\\site-packages\\pandas\\core\\internals\\construction.py\u001b[0m in \u001b[0;36mextract_index\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m    389\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    390\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mindexes\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mraw_lengths\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 391\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"If using all scalar values, you must pass an index\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    392\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    393\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhave_series\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: If using all scalar values, you must pass an index"
     ]
    }
   ],
   "source": [
    "# convert inputs to a pandas dataframe\n",
    "start = pd.to_datetime(interval_start, format=\"%d/%m/%Y\")\n",
    "end = pd.to_datetime(interval_end, format=\"%d/%m/%Y\")\n",
    "\n",
    "interval = pd.date_range(start, end, freq=\"1H\")\n",
    "interval = str(interval) # saving as csv converts datetimes to str, needs to match other datasets\n",
    "interval_df = pd.DataFrame({\"datetime\":interval})\n",
    "\n",
    "#interval_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d85bcb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the datasets\n",
    "path_to_data = \"C:/Users/Owner/Wildfire_Smoke_Mckendry/data/out_data/\"\n",
    "\n",
    "aeronet = pd.read_csv(path_to_data + \"aeronet_aod.csv\")\n",
    "hazmap = pd.read_csv(path_to_data + \"hazmap_wask.csv\")\n",
    "fluxes = pd.read_csv(path_to_data + \"OBS_ameriflux.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5340612f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#interval_df[\"datetime\"].dtype\n",
    "type(hazmap[\"datetime\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4743b76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d382dd2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1777a603",
   "metadata": {},
   "outputs": [],
   "source": [
    "# join all dataframes by date inside of spec interval\n",
    "full_dataset = interval_df.merge(aeronet, how=\"left\", on=\"datetime\")\n",
    "full_dataset = full_dataset.merge(hazmap, how=\"left\", on=\"datetime\")\n",
    "full_dataset = full_dataset.merge(fluxes, how=\"left\", on=\"datetime\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fce6852",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dataset.to_csv(\"C:/Users/Owner/Wildfire_Smoke_Mckendry/data/out_data/drf_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999b73ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots(figsize=(15,3))\n",
    "\n",
    "# hazmap\n",
    "hazmap.plot(\"datetime\", \"smoke\", ax=ax1, alpha=0.7, legend=False)\n",
    "plt.yticks(ticks=[5,16,27], labels=[\"low\", \"med\", \"high\"])\n",
    "ax1.set_ylabel(\"Hazmap Smoke Level\")\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "aeronet.plot(\"datetime\", \"AOD_500nm\", ax=ax2, color=\"orange\", alpha=0.7, legend=False)\n",
    "ax2.set_ylabel(\"Aeronet AOD\")\n",
    "\n",
    "ax3 = ax1.twinx()\n",
    "fluxes.plot(\"datetime\", \"RECO_PI\", ax=ax3, color=\"red\", alpha=0.7, legend=False)\n",
    "ax3.set_ylabel(\"Ameriflux RECO\")\n",
    "\n",
    "fig.suptitle(\"Comparing AERONET AOD Measurements to Wildfire HMS Smoke Data\")\n",
    "plt.figlegend(loc=(0.84,0.64));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b79b0481",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots(figsize=(15,3))\n",
    "\n",
    "# hazmap\n",
    "full_dataset.plot.area(\"datetime\", \"smoke\", alpha=0.7, sharex=True, legend=False)\n",
    "plt.yticks(ticks=[5,16,27], labels=[\"low\", \"med\", \"high\"])\n",
    "ax1.set_ylabel(\"Hazmap Smoke Level\")\n",
    "\n",
    "full_dataset.plot.area(\"datetime\", \"AOD_500nm\", color=\"orange\", alpha=0.7, sharex=True, legend=False)\n",
    "ax1.set_ylabel(\"Aeronet AOD\")\n",
    "\n",
    "#ax3 = ax1.twinx()\n",
    "\n",
    "\n",
    "fig.suptitle(\"Comparing AERONET AOD Measurements to Wildfire HMS Smoke Data\")\n",
    "plt.figlegend(loc=(0.84,0.7));"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
